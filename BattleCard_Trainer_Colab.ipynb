{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro"
            },
            "source": [
                "# Pipeline de Entrenamiento Aut\u00f3nomo: Agente Generador de Casos Cl\u00ednicos (AI-Med)\n",
                "Este notebook est\u00e1 dise\u00f1ado para ejecutarse en Google Colab con una GPU T4. Entrenar\u00e1 un modelo Llama-3.1-8B para generar BattleCards basadas estrictamente en contexto m\u00e9dico extra\u00eddo de NotebookLM."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "drive_mount_early"
            },
            "source": [
                "# MONTAR DRIVE AL INICIO\n",
                "# Esto asegura que tus archivos no se pierdan si se desconecta el entorno.\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "save_path = '/content/drive/MyDrive/AI_Med_Training'\n",
                "if not os.path.exists(save_path):\n",
                "    os.makedirs(save_path)\n",
                "print(f'\u2705 Los resultados se guardar\u00e1n autom\u00e1ticamente en: {save_path}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "md1"
            },
            "source": [
                "## Celda 1: Preparaci\u00f3n del Entorno Acelerado\n",
                "Instalamos Unsloth y dependencias."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "code1"
            },
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes datasets"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "md2"
            },
            "source": [
                "## Celda 2: Inicializaci\u00f3n del Modelo Base Cl\u00ednico\n",
                "Cargamos `Llama-3.1-8B-Instruct` a 4 bits."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "code2"
            },
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "max_seq_length = 4096\n",
                "dtype = None\n",
                "load_in_4bit = True\n",
                "\n",
                "# Modelo Base recomendado para tareas de estructuraci\u00f3n profunda\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dtype = dtype,\n",
                "    load_in_4bit = load_in_4bit,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "md3"
            },
            "source": [
                "## Celda 3: Configuraci\u00f3n de Adaptadores LoRA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "code3"
            },
            "outputs": [],
            "source": [
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 32,\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
                "    lora_alpha = 32,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = \"unsloth\",\n",
                "    random_state = 1234,\n",
                "    use_rslora = True,\n",
                "    loftq_config = None,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "md4"
            },
            "source": [
                "## Celda 4: Estructuraci\u00f3n Estricta del Prompt y Dataset\n",
                "\u26a0\ufe0f **IMPORTANTE:** Sube tu archivo `dataset_casos_clinicos.jsonl` a los archivos de Colab (panel izquierdo) antes de ejecutar esta celda."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "code4"
            },
            "outputs": [],
            "source": [
                "battlecard_prompt = \"\"\"Eres el Agente Generador Experto de AI-Med Learning.\n",
                "Tu \u00daNICA tarea es extraer la informaci\u00f3n m\u00e9dica provista en el [Contexto Cient\u00edfico] (previamente extra\u00eddo v\u00eda MCP de NotebookLM) y estructurarla de forma estricta en una [BattleCard] compatible con la plataforma del usuario.\n",
                "Bajo NINGUNA circunstancia puedes inventar datos que no est\u00e9n en el contexto. Si algo no est\u00e1, no lo incluyas.\n",
                "\n",
                "### Tema Solicitado:\n",
                "{}\n",
                "\n",
                "### Contexto Cient\u00edfico (Input desde MCP NotebookLM):\n",
                "{}\n",
                "\n",
                "### BattleCard (Salida Estructurada):\n",
                "{}\"\"\"\n",
                "\n",
                "EOS_TOKEN = tokenizer.eos_token\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    temas    = examples[\"tema\"]\n",
                "    contextos = examples[\"contexto_mcp\"]\n",
                "    salidas  = examples[\"casos_generados\"]\n",
                "    texts = []\n",
                "    \n",
                "    for tema, contexto, salida in zip(temas, contextos, salidas):\n",
                "        text = battlecard_prompt.format(tema, contexto, salida) + EOS_TOKEN\n",
                "        texts.append(text)\n",
                "    return { \"text\" : texts, }\n",
                "\n",
                "from datasets import load_dataset\n",
                "# Cargar la data estructurada de BattleCards\n",
                "dataset = load_dataset(\"json\", data_files=\"dataset_casos_clinicos.jsonl\", split=\"train\")\n",
                "dataset = dataset.map(formatting_prompts_func, batched = True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "md5"
            },
            "source": [
                "## Celda 5: Entrenamiento Especializado\n",
                "Dar\u00e1 inicio al Fine-Tuning de los par\u00e1metros seleccionados con optimizador de 8-bit para cuidar la VRAM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "code5"
            },
            "outputs": [],
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model = model,\n",
                "    tokenizer = tokenizer,\n",
                "    train_dataset = dataset,\n",
                "    dataset_text_field = \"text\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dataset_num_proc = 2,\n",
                "    packing = False,\n",
                "    args = TrainingArguments(\n",
                "        per_device_train_batch_size = 2,\n",
                "        gradient_accumulation_steps = 4,\n",
                "        warmup_steps = 10,\n",
                "        max_steps = 100, # Reducido un poco para recuperaci\u00f3n r\u00e1pida\n",
                "        learning_rate = 1.5e-4,\n",
                "        fp16 = not torch.cuda.is_bf16_supported(),\n",
                "        bf16 = torch.cuda.is_bf16_supported(),\n",
                "        logging_steps = 5,\n",
                "        optim = \"adamw_8bit\",\n",
                "        weight_decay = 0.05,\n",
                "        lr_scheduler_type = \"cosine\",\n",
                "        seed = 1234,\n",
                "        output_dir = '/content/drive/MyDrive/AI_Med_Training/outputs', # GUARDADO AUTOM\u00c1TICO\n",
                "    ),\n",
                ")\n",
                "\n",
                "# Inicia la magia\n",
                "trainer_stats = trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "md6"
            },
            "source": [
                "## Celda 6: Validaci\u00f3n Anti-Alucinaci\u00f3n (Prueba de Cordura)\n",
                "Forzamos al modelo a crear una tarjeta en base a un texto de prueba para ver c\u00f3mo maneja el markdown en vivo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "code6"
            },
            "outputs": [],
            "source": [
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "tema_prueba = \"Cetoacidosis Diab\u00e9tica (CAD)\"\n",
                "contexto_prueba = \"Las gu\u00edas 2024 recomiendan bolos de fluidos iniciales seguidos de insulina IV a 0.1 U/kg/hr. Criterios de resoluci\u00f3n: Glucosa < 200, HCO3 >= 18.\"\n",
                "\n",
                "inputs = tokenizer(\n",
                "[\n",
                "    battlecard_prompt.format(\n",
                "        tema_prueba,\n",
                "        contexto_prueba,\n",
                "        \"\", # Dejamos este vac\u00edo para que la IA complete la respuesta estructuradamente\n",
                "    )\n",
                "], return_tensors = \"pt\").to(\"cuda\")\n",
                "\n",
                "from transformers import TextStreamer\n",
                "text_streamer = TextStreamer(tokenizer)\n",
                "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024, temperature=0.1, do_sample=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "md7"
            },
            "source": [
                "## Celda 7: Exportaci\u00f3n a GGUF y Guardado en Google Drive\n",
                "Fusiona los adaptadores, comprime a formato q4_k_m (ideal para Mac M3) y lo guarda directamente en Google Drive para evitar fallos por archivos grandes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "code7"
            },
            "outputs": [],
            "source": [
                "# EXPORTACI\u00d3N FINAL A DRIVE\n",
                "# Como ya montamos el drive al inicio, esto es directo.\n",
                "model.save_pretrained_gguf(\"AI_Med_Generator_V1\", tokenizer, quantization_method = \"q4_k_m\")\n",
                "\n",
                "import shutil\n",
                "import os\n",
                "\n",
                "try:\n",
                "    gguf_file = [f for f in os.listdir(\"AI_Med_Generator_V1\") if f.endswith(\".gguf\")][0]\n",
                "    source_path = os.path.join(\"AI_Med_Generator_V1\", gguf_file)\n",
                "    destination_path = f\"/content/drive/MyDrive/AI_Med_Training/{gguf_file}\"\n",
                "    \n",
                "    print(f\"Copiando {source_path} a tu Google Drive... (Esto tomar\u00e1 unos minutos)\")\n",
                "    shutil.copy2(source_path, destination_path)\n",
                "    print(f\"\u00a1\u00c9XITO TOTAL! El archivo ya est\u00e1 seguro en tu Drive: AI_Med_Training/{gguf_file}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error: {e}\")"
            ]
        }
    ]
}